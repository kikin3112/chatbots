{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import textwrap\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(find_dotenv())\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elegir si enseñar o recordar (evita el uso de tokens en OpenAI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Enseñar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de documentos y extracción de información\n",
    "# (asegúrate de que haya PDFs en la carpeta documentos)\n",
    "documents = DirectoryLoader('./documentos/', glob=\"./*.pdf\", loader_cls=PyPDFLoader).load()\n",
    "# Tratameinto de caracteres indeseados\n",
    "for d in documents:\n",
    "    d.page_content = d.page_content.replace('\\n', ' ').replace('\\t', ' ')\n",
    "# Separador de texto (límita el texto para que sea soportado por el LLM)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=3000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "textos = text_splitter.split_documents(documents)\n",
    "# Se instruye (creación de un knowledge-base) con la info\n",
    "# proporcionada\n",
    "persist_directory = input('¿Cómo se llama este knoledge-base?: ') + '_kb'\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectordb = Chroma.from_documents(documents=textos, \n",
    "                                 embedding=embedding,\n",
    "                                 persist_directory=persist_directory)\n",
    "# Se guarda el conocimiento generado en el directorio actual\n",
    "# para ser usado sin tener que instruir al LLM\n",
    "vectordb.persist()\n",
    "vectordb = None\n",
    "os.system(f'zip -r db.zip ./{persist_directory}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Recordar (si y solo si Enseñar=Done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can load the persisted database from disk, and use it as normal. \n",
    "persist_directory = input('¿Qué knoledge-base deseas usar?: ') + '_kb'\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectordb = Chroma(persist_directory=persist_directory, \n",
    "                  embedding_function=embedding)\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 5})\n",
    "#retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se especifica el LLM de OpenAI\n",
    "turbo_llm = ChatOpenAI(\n",
    "    temperature=0.7,\n",
    "    model_name='gpt-3.5-turbo-0125'\n",
    ")\n",
    "# Se personaliza el LLM\n",
    "template = \"\"\"\n",
    "Eres Prometeo, un asistente personal de lectura que habla Español.\n",
    "\n",
    "Tu tarea consiste en:\n",
    "\n",
    "1. Ser carismático y ofrecer información sobre ti y tus funciones.\n",
    "\n",
    "2. Leer detalladamente la información proporcionada en documentos\n",
    "de texto en formato PDF, para ser capaz de dar respuestas a cualquier tipo de pregunta,\n",
    "en especial a preguntas puntuales.\n",
    "\n",
    "SIEMPRE debes responder con base al contexto proporcionado aqui: {context}.\n",
    "        \n",
    "Si no sabes la respuesta, puedes decir: 'No sé'.\n",
    "\n",
    "NUNCA hables del contexto.\n",
    "\n",
    "Teniendo lo anterior en cuenta, responde la siguiente pregunta: {question}\n",
    "\"\"\"\n",
    "\n",
    "prometeo_prompt = PromptTemplate(\n",
    "    template=template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "# Creación de cadena\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=turbo_llm,\n",
    "                                       chain_type=\"stuff\",\n",
    "                                       retriever=retriever,\n",
    "                                       chain_type_kwargs={\"prompt\": prometeo_prompt},\n",
    "                                       return_source_documents=True)\n",
    "# Formato y referencias de las respuestas\n",
    "def wrap_text_preserve_newlines(text, width=70):\n",
    "    lines = text.split('\\n')\n",
    "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "    wrapped_text = '\\n'.join(wrapped_lines)\n",
    "    return wrapped_text\n",
    "\n",
    "def process_llm_response(llm_response):\n",
    "    print(wrap_text_preserve_newlines(llm_response['result']))\n",
    "    print('\\nReferencias:')\n",
    "    for source in llm_response[\"source_documents\"]:\n",
    "        print(source.metadata['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo\n",
    "query = input(\"Hazme una pregunta: \")\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Demo break down\n",
    "query = query#input(\"Hazme una pregunta: \")\n",
    "llm_response = qa_chain(query)\n",
    "llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo\n",
    "query = input(\"Hazme una pregunta: \")\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Demo break down\n",
    "query = query#input(\"Hazme una pregunta: \")\n",
    "llm_response = qa_chain(query)\n",
    "llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(qa_chain.combine_documents_chain.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
